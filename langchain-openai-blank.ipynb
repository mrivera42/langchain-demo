{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235bb9af",
   "metadata": {},
   "source": [
    "# LangChain Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf9556",
   "metadata": {},
   "source": [
    "## What is LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b58ed",
   "metadata": {},
   "source": [
    "LangChain is a library that provides a lot of helpful components for working with LLMs. These include:\n",
    "* Model I/O\n",
    "* Memory \n",
    "* Data connection \n",
    "* Chains\n",
    "* Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6bb611",
   "metadata": {},
   "source": [
    "## Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee0ae5",
   "metadata": {},
   "source": [
    "Model I/O deals with the components for interfacing with the LLM models themselves, as well as formatting the input prompt and getting the desired outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b940f6",
   "metadata": {},
   "source": [
    "#### Use LangChains interface to make calls to LLM providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63563a",
   "metadata": {},
   "source": [
    "LangChain provides inferfaces for making calls to LLM. This can either:\n",
    "1. a call to an LLM provider (OpenAI, Cohere, PaLM, HuggingFace, etc)\n",
    "2. a custom LLM wrapper for your own LLM. \n",
    "\n",
    "In this example, we will use OpenAI inferface to interfact with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ba303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93dc5e8",
   "metadata": {},
   "source": [
    "#### Use prompt templates to format input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94d887",
   "metadata": {},
   "source": [
    "Prompt templates are a way to parameterize and reuse prompts. It's basically a wrapper class for an f-string. So you can specify the parameters at for that specific instance instead of having to redeclare them throughout your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a51cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa69b50e",
   "metadata": {},
   "source": [
    "#### Use Output Parsers to format LLM response into desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b54c0",
   "metadata": {},
   "source": [
    "The problem with LLM outputs - they only output strings. So even if we ask for a specific format (json, etc) the LLM can only return a stirng representation of that. Output Parsers are classes to help structure LLM responses.They work in 2 steps: \n",
    "1) Use the output parser to get format instructions that you will add to the prompt. This tells the LLM format the output in a way that is parsable by the output parser\n",
    "2) Use the output parser to parse the LLM output into the desired structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92094e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dda2ce8",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fd8ec",
   "metadata": {},
   "source": [
    "Chains are basically functionality to string together mutiple LLMs and prompts to achieve a desired behavior. The simplest chain is an LLMChain (LLM + Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c09108",
   "metadata": {},
   "source": [
    "#### LLMChain (Model + Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d15e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9173a8b3",
   "metadata": {},
   "source": [
    "#### SimpleSequentialChain(multiple LLM Chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e483c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6185b68",
   "metadata": {},
   "source": [
    "### Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561f1ca",
   "metadata": {},
   "source": [
    "#### How do we give LLMs conversational memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22231698",
   "metadata": {},
   "source": [
    "LLMs naturally do not remember anything. They are stateless, so each call is independent and isolated from previous ones. We can only make them \"appear\" to have memory by injecting the conversation history into the prompt as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dfcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cfa1ef",
   "metadata": {},
   "source": [
    "Sometimes we want to set a limit on the number of tokens we pass into the LLM. This could be to reduce cost or prevent LLM performance from degrading, or to make sure we are under the token limit. There are several methods to limit the conversation history:\n",
    "* ConversationBufferWindowMemory - Only keep the last k conversational exchanges (each conversational exchange is a message from you + a message from the chatbot)\n",
    "* ConversationalTokenBufferMeory - Only keep the last k tokens from the conversation\n",
    "* ConversationalSummaryBufferMemory - use a separate LLM to write a summary of the conversation so far as memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408bb99",
   "metadata": {},
   "source": [
    "## Data Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9dfc7",
   "metadata": {},
   "source": [
    "### What's the issue with long prompts? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04973c54",
   "metadata": {},
   "source": [
    "The issue is that LLMs can only take in a fixed token size. So if we want to use documents, or lots of data as context, it wont work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea88f4",
   "metadata": {},
   "source": [
    "### Solution: Chunking + Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab4b11",
   "metadata": {},
   "source": [
    "The solution is to break the document into smaller chunks, and only inject the chunks that are relevant to the question into the prompt. Vector Databases are used to store embeddings of the chunks so we can perform fast similarity search and retrieval of the relevant chunks. The workflow looks like this: \n",
    "1. Load in documents\n",
    "2. Split document into chunks\n",
    "3. Create embeddings of those chunks\n",
    "4. Index those chunks into a Vector Database \n",
    "5. Use similarity search to retrieve the relevant chunks for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "634ef94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.tex1t_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b385073",
   "metadata": {},
   "source": [
    "####  Step 1: Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66294d",
   "metadata": {},
   "source": [
    "#### Step 2: Split into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d89c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0,separator=\"\\n\")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "282d4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Birthdays in the family:', metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Zarah's birthday is in February\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Max's birthday is in April\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- James' birthday is in May\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Mom's birthday is in July\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Dad's birthday is in October\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Zyde's birthday is in December\", metadata={'source': './birthday_ideas.txt'})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11971d9d",
   "metadata": {},
   "source": [
    "#### Step 3: Create embedding function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772c96f",
   "metadata": {},
   "source": [
    "#### Step 4: Create vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cd032",
   "metadata": {},
   "source": [
    "#### Step 5: Similarity Search for relevant chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4d476",
   "metadata": {},
   "source": [
    "#### Create LLM Chain to use Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "68b868ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b6a93cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the following question:\n",
      "```When is James's birthday?```\n",
      "Using ONLY the following context:\n",
      "```[Document(page_content=\"- James' birthday is in May\", metadata={'source': './birthday_ideas.txt'}), Document(page_content='Birthdays in the family:', metadata={'source': './birthday_ideas.txt'}), Document(page_content=\"- Max's birthday is in April\", metadata={'source': './birthday_ideas.txt'}), Document(page_content=\"- Dad's birthday is in October\", metadata={'source': './birthday_ideas.txt'})]```\n",
      "If the context does not provide relevant information, just say \"I don't know\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "James's birthday is in May.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "template = \"\"\"\n",
    "Answer the following question:\n",
    "```{question}```\n",
    "Using ONLY the following context:\n",
    "```{context}```\n",
    "If the context does not provide relevant information, just say \"I don't know\"\n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True\n",
    ")\n",
    "question = \"When is James's birthday?\"\n",
    "context = db.similarity_search(question)\n",
    "print(chain.run(question=question,context=context,verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c36064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
