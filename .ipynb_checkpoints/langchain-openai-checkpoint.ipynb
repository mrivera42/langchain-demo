{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235bb9af",
   "metadata": {},
   "source": [
    "# LangChain Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf9556",
   "metadata": {},
   "source": [
    "## What is LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b58ed",
   "metadata": {},
   "source": [
    "LangChain is a library that provides a lot of helpful components for working with LLMs. These include:\n",
    "* Model I/O\n",
    "* Memory \n",
    "* Data connection \n",
    "* Chains\n",
    "* Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6bb611",
   "metadata": {},
   "source": [
    "## Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911ec50",
   "metadata": {},
   "source": [
    "Model I/O deals with the components for interfacing with the LLM models themselves, as well as formatting the input prompt and getting the desired outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b940f6",
   "metadata": {},
   "source": [
    "#### Use LangChains interface to make calls to LLM providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d06785",
   "metadata": {},
   "source": [
    "LangChain provides inferfaces for making calls to LLM. This can either:\n",
    "1. a call to an LLM provider (OpenAI, Cohere, PaLM, HuggingFace, etc)\n",
    "2. a custom LLM wrapper for your own LLM. \n",
    "\n",
    "In this example, we will use OpenAI inferface to interfact with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e8c7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b1d0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "llm(\"Why did the chicken cross the road?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dc5e8",
   "metadata": {},
   "source": [
    "#### Use prompt templates to format input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601bb57",
   "metadata": {},
   "source": [
    "Prompt templates are a way to parameterize and reuse prompts. It's basically a wrapper class for an f-string. So you can specify the parameters at for that specific instance instead of having to redeclare them throughout your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5ff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9adf585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dear Dad,\n",
      "\n",
      "I hope this letter finds you well. I'm writing to tell you all about my trip to Las Vegas.\n",
      "\n",
      "The city is amazing! I've loved every minute of my stay here. The lights, the sounds, the smells - it's like being in a fantasy world. Everywhere I go, I find something new and exciting. I've been to some of the biggest casinos and hotels, and I've seen a lot of shows. I'm also trying my luck at the slots - so far, I'm up a few bucks!\n",
      "\n",
      "The people in Las Vegas are really friendly, and the nightlife is incredible. I've been out to a few clubs and bars, and it's been a lot of fun. Plus, I've been able to take some awesome photos to show you.\n",
      "\n",
      "Overall, it's been an amazing experience. I can't wait to tell you all about it when I get home.\n",
      "\n",
      "Love,\n",
      "Max\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Write a letter to {recipient} about {subject} from {sender}.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"recipient\", \"subject\",\"sender\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "letter_to_dad = prompt.format(recipient='Dad',subject='Las Vegas',sender='Max')\n",
    "\n",
    "print(llm(letter_to_dad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69b50e",
   "metadata": {},
   "source": [
    "#### Use Output Parsers to format LLM response into desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcbdf1",
   "metadata": {},
   "source": [
    "The problem with LLM outputs - they only output strings. So even if we ask for a specific format (json, etc) the LLM can only return a stirng representation of that. Output Parsers are classes to help structure LLM responses.They work in 2 steps: \n",
    "1) Use the output parser to get format instructions that you will add to the prompt. This tells the LLM format the output in a way that is parsable by the output parser\n",
    "2) Use the output parser to parse the LLM output into the desired structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f33a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\tsender: \"Max\",\n",
      "\tlocation: \"Las Vegas\",\n",
      "\trecipient: \"Dad\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following letter separated by backticks: \n",
    "\n",
    "```{letter}```\n",
    "\n",
    "Extract the following information: \n",
    "\n",
    "sender: Who is this letter from?\n",
    "location: What location is the letter about? \n",
    "recipient: Who is this letter addressed to?\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sender\n",
    "location\n",
    "recipient\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "message = prompt.format(letter=letter_to_dad)\n",
    "response = llm(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88cfe27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f93fe35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d761206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_schema = ResponseSchema(name=\"sender\",description=\"Who is this letter from?\")\n",
    "location_schema = ResponseSchema(name=\"location\",description=\"What location is the letter about?\")\n",
    "recipient_schema = ResponseSchema(name=\"recipient\",description=\"Who is this letter addressed to?\")\n",
    "response_schemas=[sender_schema, location_schema, recipient_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be46f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"sender\": string  // Who is this letter from?\n",
      "\t\"location\": string  // What location is the letter about?\n",
      "\t\"recipient\": string  // Who is this letter addressed to?\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c86e6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```json\n",
      "{\n",
      "\t\"sender\": \"Max\", \n",
      "\t\"location\": \"Las Vegas\",\n",
      "\t\"recipient\": \"Dad\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following letter separated by backticks: \n",
    "\n",
    "```{letter}```\n",
    "\n",
    "Extract the following information: \n",
    "\n",
    "sender: Who is this letter from?\n",
    "location: What location is the letter about? \n",
    "recipient: Who is this letter addressed to?\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sender\n",
    "location\n",
    "recipient\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "message = prompt.format(letter=letter_to_dad, format_instructions=format_instructions)\n",
    "response = llm(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a489d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "925a7634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sender': 'Max', 'location': 'Las Vegas', 'recipient': 'Dad'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4abe813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9501aff",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff1090",
   "metadata": {},
   "source": [
    "Chains are basically functionality to string together mutiple LLMs and prompts to achieve a desired behavior. The simplest chain is an LLMChain (LLM + Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a7704",
   "metadata": {},
   "source": [
    "#### LLMChain (Model + Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a3ea9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "template = \"\"\"\n",
    "Create a trendy company name for a company that works on {topic}.\n",
    "\"\"\"\n",
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cbb6cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JiuJitsuAiTech.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(\"AI and Jiu Jitsu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SimpleSequentialChain(LLMChain + LLMChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3230bb",
   "metadata": {},
   "source": [
    "### Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231c823",
   "metadata": {},
   "source": [
    "#### How do we give LLMs conversational memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf396e",
   "metadata": {},
   "source": [
    "LLMs naturally do not remember anything. They are stateless, so each call is independent and isolated from previous ones. We can only make them \"appear\" to have memory by injecting the conversation history into the prompt as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ed31065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72109249",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90a64bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Max\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Max, my name is AI. It's nice to meet you. What brings you here today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f601c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Max\n",
      "AI:  Hi Max, my name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said your name is Max. Is that correct?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5edcab",
   "metadata": {},
   "source": [
    "Sometimes we want to set a limit on the number of tokens we pass into the LLM. This could be to reduce cost or prevent LLM performance from degrading, or to make sure we are under the token limit. There are several methods to limit the conversation history:\n",
    "* ConversationBufferWindowMemory - Only keep the last k conversational exchanges (each conversational exchange is a message from you + a message from the chatbot)\n",
    "* ConversationalTokenBufferMeory - Only keep the last k tokens from the conversation\n",
    "* ConversationalSummaryBufferMemory - use a separate LLM to write a summary of the conversation so far as memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3d003",
   "metadata": {},
   "source": [
    "## Data Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b2c73",
   "metadata": {},
   "source": [
    "### What's the issue with long prompts? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc149df9",
   "metadata": {},
   "source": [
    "The issue is that LLMs can only take in a fixed token size. So if we want to use documents, or lots of data as context, it wont work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b89ba3",
   "metadata": {},
   "source": [
    "### Solution: Chunking + Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bb148",
   "metadata": {},
   "source": [
    "The solution is to break the document into smaller chunks, and only inject the chunks that are relevant to the question into the prompt. Vector Databases are used to store embeddings of the chunks so we can perform fast similarity search and retrieval of the relevant chunks. The workflow looks like this: \n",
    "1. Load in documents\n",
    "2. Split document into chunks\n",
    "3. Create embeddings of those chunks\n",
    "4. Index those chunks into a Vector Database \n",
    "5. Use similarity search to retrieve the relevant chunks for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b785125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b385073",
   "metadata": {},
   "source": [
    "####  Step 1: Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86756c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./birthday_ideas.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d4f86a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Birthdays in the family: \\n- Zarah's birthday is in February \\n- Max's birthday is in April \\n- James' birthday is in May \\n- Mom's birthday is in July \\n- Dad's birthday is in October \\n- Zyde's birthday is in December \", metadata={'source': './birthday_ideas.txt'})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66294d",
   "metadata": {},
   "source": [
    "#### Step 2: Split into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d89c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0,separator=\"\\n\")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "282d4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Birthdays in the family:', metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Zarah's birthday is in February\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Max's birthday is in April\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- James' birthday is in May\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Mom's birthday is in July\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Dad's birthday is in October\", metadata={'source': './birthday_ideas.txt'}),\n",
       " Document(page_content=\"- Zyde's birthday is in December\", metadata={'source': './birthday_ideas.txt'})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78d8ed",
   "metadata": {},
   "source": [
    "Embed chunks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcca54",
   "metadata": {},
   "source": [
    "#### Step 3: Create embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f1fee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f0bcd8804c4198aee10e900d0feff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44750b9197646ba82df47c5de71e588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9f971e59fa41aca90ca5831674a9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6df56c2b608477789098712f26ca3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f9a59998314c27a82ddea04b0cc7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868d54853e742719b9881cbed86f374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eab3e68c3274eafaf60a6ef367635f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cd36c34a0340e9af42ce31ff5cd18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3062f053214af0b808780586bf8509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a7ccd188e64bb7834fb35720ad9a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef61bac52694832847ad7453b0dc7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d676a5dc04f868849de89b51c568d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b881a3361ef40d88f35a18438e2cc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea409cf590ff41b5ba41a846324a8345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7545b7",
   "metadata": {},
   "source": [
    "#### Step 4: Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f491e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e148c",
   "metadata": {},
   "source": [
    "#### Step 5: Similarity Search for relevant chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec4b6ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"- Dad's birthday is in October\", metadata={'source': './birthday_ideas.txt'})]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When is Dad's birthday?\"\n",
    "relevant_chunk = db.similarity_search(query, k=1)\n",
    "relevant_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21bf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
